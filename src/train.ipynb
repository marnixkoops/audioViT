{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "\n",
    "import librosa\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import timm\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from joblib import Parallel, delayed\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sns.set(\n",
    "    rc={\n",
    "        \"figure.figsize\": (8, 4),\n",
    "        \"figure.dpi\": 240,\n",
    "    }\n",
    ")\n",
    "sns.set_style(\"darkgrid\", {\"axes.grid\": False})\n",
    "sns.set_context(\"paper\", font_scale=0.6)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    experiment_name = \"augment\"\n",
    "    data_path = \"../data\"\n",
    "\n",
    "    debug_run = True  # run on 250 rows\n",
    "    experiment_run = False  # run on a stratified 50% data sample\n",
    "    competition_run = False  # run on full data\n",
    "\n",
    "    normalize_waveform = True\n",
    "    sample_rate = 32000\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "    window_length = None\n",
    "    melspec_hres = 128\n",
    "    melspec_wres = 312\n",
    "    freq_min = 20\n",
    "    freq_max = 16000\n",
    "    log_scale_power = 2\n",
    "    max_decibels = 80\n",
    "    frame_duration = 5\n",
    "    frame_rate = sample_rate / hop_length\n",
    "\n",
    "    vit_b0 = \"efficientvit_b0.r224_in1k\"\n",
    "    # vit_b1 = \"efficientvit_b1.r224_in1k\"\n",
    "    # vit_b1 = \"efficientvit_b1.r288_in1\"\n",
    "    # effnet_b0 = \"tf_efficientnetv2_b0.in1k\"\n",
    "    # effnet_b1 = \"tf_efficientnetv2_b1.in1k\"\n",
    "    # vit_m0 = \"efficientvit_m0.r224_in1k\"\n",
    "    # vit_m1 = \"efficientvit_m1.r224_in1k\"\n",
    "    backbone = vit_b0\n",
    "\n",
    "    num_classes = 182\n",
    "    add_secondary_labels = True\n",
    "    label_smoothing = 0.0\n",
    "\n",
    "    accelerator = \"gpu\"\n",
    "    precision = \"16-mixed\"\n",
    "    n_workers = os.cpu_count() - 4\n",
    "\n",
    "    n_epochs = 25\n",
    "    batch_size = 128\n",
    "    val_ratio = 0.20\n",
    "\n",
    "    lr_min = 1e-7\n",
    "    lr_max = 1e-4\n",
    "    weight_decay = 1e-6\n",
    "    fused_adamw = True\n",
    "    patience = 10\n",
    "\n",
    "    timestamp = datetime.now().replace(microsecond=0)\n",
    "    run_tag = f\"{timestamp}_{backbone}_{experiment_name}_val_{val_ratio}_lr_{lr_max}_decay_{weight_decay}\"\n",
    "\n",
    "    if debug_run:\n",
    "        run_tag = f\"{timestamp}_{backbone}_debug\"\n",
    "        lr_max = 1e-2\n",
    "        accelerator = \"cpu\"\n",
    "        n_epochs = 10\n",
    "        batch_size = 32\n",
    "        fused_adamw = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_logger():\n",
    "    handlers = [\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(f\"../logs/{cfg.run_tag}.log\"),\n",
    "    ]\n",
    "\n",
    "    if cfg.debug_run:\n",
    "        handlers = [logging.StreamHandler()]\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\" %(asctime)s [%(threadName)s] ðŸ¦â€ðŸ”¥ %(message)s\",\n",
    "        handlers=handlers,\n",
    "        force=True,  # reconfigure root logger, in case of rerunning -> ensures new file\n",
    "    )\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_config(cfg) -> None:\n",
    "    cfg_dictionary = {\n",
    "        key: value\n",
    "        for key, value in cfg.__dict__.items()\n",
    "        if not key.startswith(\"__\") and not callable(key)\n",
    "    }\n",
    "    logger.info(f\"{'â€”' * 80}\")\n",
    "    logger.info(f\"Config: \\n{pformat(cfg_dictionary, indent=1)}\")\n",
    "    return cfg_dictionary\n",
    "\n",
    "\n",
    "def load_metadata(data_path: str) -> pd.DataFrame:\n",
    "    logger.info(f\"Loading prepared dataframes from {data_path}\")\n",
    "    model_input_df = pd.read_csv(f\"{data_path}/model_input_df.csv\")\n",
    "    sample_submission = pd.read_csv(f\"{data_path}/sample_submission.csv\")\n",
    "\n",
    "    if cfg.debug_run:\n",
    "        logger.info(\"Running debug: sampling data to 10 species and 250 samples\")\n",
    "        top_10_labels = model_input_df[\"primary_label\"].value_counts()[0:25].index\n",
    "        model_input_df = model_input_df[\n",
    "            model_input_df[\"primary_label\"].isin(top_10_labels)\n",
    "        ]\n",
    "        model_input_df = model_input_df.sample(1000).reset_index(drop=True)\n",
    "\n",
    "    if cfg.experiment_run:\n",
    "        logger.info(\"Running experiment: stratified sampling 50% of data\")\n",
    "        model_input_df, _ = train_test_split(\n",
    "            model_input_df,\n",
    "            test_size=0.50,\n",
    "            stratify=model_input_df[\"primary_label\"],\n",
    "            shuffle=True,\n",
    "            random_state=None,\n",
    "        )\n",
    "\n",
    "    logger.info(f\"Dataframe shape: {model_input_df.shape}\")\n",
    "\n",
    "    return model_input_df, sample_submission\n",
    "\n",
    "\n",
    "def read_waveform(filename: str) -> np.ndarray:\n",
    "    filepath = f\"{cfg.data_path}/train_windows_n5_s5_c9/{filename}\"\n",
    "    waveform, _ = librosa.load(filepath, sr=cfg.sample_rate)\n",
    "    return waveform\n",
    "\n",
    "\n",
    "def read_waveforms_parallel(model_input_df: pd.DataFrame):\n",
    "    logger.info(\"Parallel Loading waveforms\")\n",
    "    waveforms = Parallel(n_jobs=cfg.n_workers, prefer=\"threads\")(\n",
    "        delayed(read_waveform)(filename)\n",
    "        for filename in tqdm(model_input_df[\"window_filename\"], desc=\"Loading waves\")\n",
    "    )\n",
    "    logger.info(\"Finished loadeding waveforms\")\n",
    "    return waveforms\n",
    "\n",
    "\n",
    "def create_label_map(submission_df: pd.DataFrame) -> dict:\n",
    "    logging.info(\"Creating label mappings\")\n",
    "    cfg.labels = submission_df.columns[1:]\n",
    "    cfg.num_classes = len(cfg.labels)\n",
    "    class_to_label_map = dict(zip(cfg.labels, np.arange(cfg.num_classes)))\n",
    "\n",
    "    return class_to_label_map\n",
    "\n",
    "\n",
    "def pad_or_crop_waveforms(waveforms: list) -> list:\n",
    "    logging.info(\"Padding or cropping waveforms to desired duration\")\n",
    "    desired_length = cfg.sample_rate * cfg.frame_duration\n",
    "\n",
    "    def _pad_or_crop(waveform: np.ndarray) -> np.ndarray:\n",
    "        length = len(waveform)\n",
    "        if length < desired_length:  # repeat if waveform too small\n",
    "            repeat_length = desired_length - length\n",
    "            waveform = np.concatenate([waveform, waveform[:repeat_length]])\n",
    "            length = len(waveform)\n",
    "\n",
    "            if length < desired_length:  # repeat if waveform still too small\n",
    "                repeat_length = desired_length - length\n",
    "                waveform = np.concatenate([waveform, waveform[:repeat_length]])\n",
    "                length = len(waveform)\n",
    "\n",
    "        if length > desired_length:  # crop if waveform is too big\n",
    "            offset = np.random.randint(0, length - desired_length)\n",
    "            waveform = waveform[offset : offset + desired_length]\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    waveforms = [_pad_or_crop(wave) for wave in tqdm(waveforms, desc=\"Padding waves\")]\n",
    "\n",
    "    return waveforms\n",
    "\n",
    "\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        waveforms: list,\n",
    "        add_secondary_labels: bool = cfg.add_secondary_labels,\n",
    "        melspec_augmentations: list = None,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.waveforms = waveforms\n",
    "        self.num_classes = cfg.num_classes\n",
    "        self.class_to_label_map = class_to_label_map\n",
    "        self.add_secondary_labels = add_secondary_labels\n",
    "\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=cfg.sample_rate,\n",
    "            n_mels=cfg.melspec_hres,\n",
    "            f_min=cfg.freq_min,\n",
    "            f_max=cfg.freq_max,\n",
    "            n_fft=cfg.n_fft,\n",
    "            hop_length=cfg.hop_length,\n",
    "            normalized=cfg.normalize_waveform,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            norm=\"slaney\",\n",
    "            mel_scale=\"slaney\",\n",
    "        )\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=cfg.max_decibels\n",
    "        )\n",
    "\n",
    "    def create_target(\n",
    "        self,\n",
    "        primary_label: str,\n",
    "        secondary_labels: list,\n",
    "        secondary_label_weight: float = 1,\n",
    "    ) -> torch.tensor:\n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        # primary_target = torch.tensor(0, dtype=torch.int64)\n",
    "\n",
    "        if primary_label != \"nocall\":\n",
    "            primary_label = self.class_to_label_map[primary_label]\n",
    "            target[primary_label] = 1\n",
    "            primary_target = torch.tensor(primary_label, dtype=torch.int64)\n",
    "\n",
    "            if self.add_secondary_labels:\n",
    "                secondary_labels = eval(secondary_labels)\n",
    "                for label in secondary_labels:\n",
    "                    if label != \"\" and label in self.class_to_label_map.keys():\n",
    "                        target[self.class_to_label_map[label]] = secondary_label_weight\n",
    "\n",
    "        binary_target = target.to(torch.int64)\n",
    "\n",
    "        return target, binary_target, primary_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waveforms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform = self.waveforms[idx]\n",
    "        primary_label = self.df.iloc[idx][\"primary_label\"]\n",
    "        secondary_labels = self.df.iloc[idx][\"secondary_labels\"]\n",
    "\n",
    "        # if waveform_augmentations:\n",
    "        #     waveform = waveform_augmentations(waveform, sample_rate=cfg.sample_rate)\n",
    "\n",
    "        waveform = torch.tensor(waveform, dtype=torch.float32).squeeze()\n",
    "\n",
    "        melspec = self.db_transform(self.mel_transform(waveform)).type(torch.uint8)\n",
    "\n",
    "        melspec = melspec - melspec.min()\n",
    "        melspec = melspec / melspec.max() * 255\n",
    "\n",
    "        target, binary_target, primary_target = self.create_target(\n",
    "            primary_label=primary_label, secondary_labels=secondary_labels\n",
    "        )\n",
    "\n",
    "        return melspec, target, binary_target, primary_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger = define_logger()\n",
    "    config_dictionary = get_config(cfg)\n",
    "\n",
    "    model_input_df, sample_submission = load_metadata(data_path=cfg.data_path)\n",
    "    class_to_label_map = create_label_map(submission_df=sample_submission)\n",
    "\n",
    "    waveforms = read_waveforms_parallel(model_input_df=model_input_df)\n",
    "    waveforms = pad_or_crop_waveforms(waveforms=waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped split on sample index to keep different windows from the same sample\n",
    "    # together if splitting randomly this can be considered as a form of leakage\n",
    "    # validating on a windowed waveform while windows of the same waveform were used for\n",
    "    # training is easier than classifying a waveform from a different sample, which is\n",
    "    # the case in practice\n",
    "\n",
    "    logger.info(f\"Splitting {len(waveforms)} waveforms into train/val: {cfg.val_ratio}\")\n",
    "    n_splits = int(round(1 / cfg.val_ratio))\n",
    "    kfold = StratifiedGroupKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    for fold_index, (train_index, val_index) in enumerate(\n",
    "        kfold.split(\n",
    "            X=model_input_df,\n",
    "            y=model_input_df[\"primary_label\"],\n",
    "            groups=model_input_df[\"sample_index\"],\n",
    "        )\n",
    "    ):\n",
    "        break  # run a single split for now\n",
    "\n",
    "    train_df = model_input_df.iloc[train_index]\n",
    "    val_df = model_input_df.iloc[val_index]\n",
    "\n",
    "    train_waveforms = [waveforms[i] for i in train_index]\n",
    "    val_waveforms = [waveforms[i] for i in val_index]\n",
    "\n",
    "    train_dataset = BirdDataset(df=train_df, waveforms=train_waveforms)\n",
    "    val_dataset = BirdDataset(df=val_df, waveforms=val_waveforms)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        drop_last=True,\n",
    "        num_workers=cfg.n_workers,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        num_workers=cfg.n_workers,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    class FocalLoss(torch.nn.Module):\n",
    "        def __init__(\n",
    "            self,\n",
    "            alpha: float = 0.25,\n",
    "            gamma: float = 2,\n",
    "            reduction: str = \"mean\",\n",
    "        ):\n",
    "            super().__init__()\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "            self.reduction = reduction\n",
    "\n",
    "        def forward(self, x, y):\n",
    "            loss = torchvision.ops.focal_loss.sigmoid_focal_loss(\n",
    "                inputs=x,\n",
    "                targets=y,\n",
    "                alpha=self.alpha,\n",
    "                gamma=self.gamma,\n",
    "                reduction=self.reduction,\n",
    "            )\n",
    "            return loss\n",
    "\n",
    "    class FocalLossBCE(torch.nn.Module):\n",
    "        def __init__(\n",
    "            self,\n",
    "            alpha: float = 0.25,\n",
    "            gamma: float = 2,\n",
    "            reduction: str = \"mean\",\n",
    "            bce_weight: float = 1.0,\n",
    "            focal_weight: float = 1.0,\n",
    "        ):\n",
    "            super().__init__()\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "            self.reduction = reduction\n",
    "            self.bce = torch.nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "            self.bce_weight = bce_weight\n",
    "            self.focal_weight = focal_weight\n",
    "\n",
    "        def forward(self, inputs, targets):\n",
    "            focall_loss = torchvision.ops.focal_loss.sigmoid_focal_loss(\n",
    "                inputs=inputs,\n",
    "                targets=targets,\n",
    "                alpha=self.alpha,\n",
    "                gamma=self.gamma,\n",
    "                reduction=self.reduction,\n",
    "            )\n",
    "            bce_loss = self.bce(inputs, targets)\n",
    "            combined_loss = self.bce_weight * bce_loss + self.focal_weight * focall_loss\n",
    "            return combined_loss\n",
    "\n",
    "    class EfficientViT(L.LightningModule):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.vit = timm.create_model(\n",
    "                cfg.backbone,\n",
    "                pretrained=True,\n",
    "                num_classes=cfg.num_classes,\n",
    "            )\n",
    "\n",
    "            self.imagenet_normalize = transforms.Normalize(\n",
    "                [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "            )\n",
    "\n",
    "            self.loss_function = FocalLoss()\n",
    "            self.loss_function = FocalLossBCE()\n",
    "            # self.loss_function = nn.CrossEntropyLoss(\n",
    "            #     label_smoothing=cfg.label_smoothing\n",
    "            # )\n",
    "\n",
    "            self.accuracy_top1 = torchmetrics.Accuracy(\n",
    "                task=\"multiclass\", num_classes=cfg.num_classes, top_k=1\n",
    "            )\n",
    "            self.accuracy_top2 = torchmetrics.Accuracy(\n",
    "                task=\"multiclass\", num_classes=cfg.num_classes, top_k=2\n",
    "            )\n",
    "            self.f1_macro = torchmetrics.F1Score(\n",
    "                task=\"multilabel\",\n",
    "                num_labels=cfg.num_classes,\n",
    "                average=\"macro\",\n",
    "                ignore_index=0,\n",
    "            )\n",
    "            self.f1_weighted = torchmetrics.F1Score(\n",
    "                task=\"multilabel\",\n",
    "                num_labels=cfg.num_classes,\n",
    "                average=\"weighted\",\n",
    "                ignore_index=0,\n",
    "            )\n",
    "            self.auroc = torchmetrics.AUROC(\n",
    "                task=\"multilabel\",\n",
    "                num_labels=cfg.num_classes,\n",
    "                average=\"macro\",\n",
    "            )\n",
    "            self.lrap = torchmetrics.classification.MultilabelRankingAveragePrecision(\n",
    "                num_labels=cfg.num_classes\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(1).expand(-1, 3, -1, -1)  # go from HxW â†’ 3xHxW\n",
    "            x = x.float() / 255\n",
    "            x = self.imagenet_normalize(x)\n",
    "            out = self.vit(x)\n",
    "\n",
    "            return out\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            x, y, y_binary, y_primary = batch\n",
    "            y_pred = self(x)\n",
    "            loss = self.loss_function(y_pred, y)\n",
    "\n",
    "            train_accuracy_top1 = self.accuracy_top1(y_pred, y_primary)\n",
    "            train_accuracy_top2 = self.accuracy_top2(y_pred, y_primary)\n",
    "            train_f1_macro = self.f1_macro(y_pred, y_binary)\n",
    "            train_lrap = self.lrap(y_pred, y_binary)\n",
    "\n",
    "            self.log(\n",
    "                \"train_loss\",\n",
    "                loss,\n",
    "                on_step=True,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"train_acc_1\",\n",
    "                train_accuracy_top1,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"train_acc_2\",\n",
    "                train_accuracy_top2,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"train_f1_macro_ignore\",\n",
    "                train_f1_macro,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"train_lrap\",\n",
    "                train_lrap,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "\n",
    "            return loss\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            x_val, y_val, y_binary_val, y_primary_val = batch\n",
    "            y_pred = self(x_val)\n",
    "            val_loss = self.loss_function(y_pred, y_val)\n",
    "\n",
    "            logger.info(f\"\\n y_pred: {y_pred} \\n\")\n",
    "\n",
    "            logger.info(f\"\\n y_pred.softmax(dim=1): {y_pred.softmax(dim=1)} \\n\")\n",
    "\n",
    "            val_accuracy_top1 = self.accuracy_top1(y_pred, y_primary_val)\n",
    "            val_accuracy_top2 = self.accuracy_top2(y_pred, y_primary_val)\n",
    "            val_f1_weighted = self.f1_weighted(y_pred, y_binary_val)\n",
    "            val_f1_macro = self.f1_macro(y_pred, y_binary_val)\n",
    "            val_auroc = self.auroc(y_pred, y_binary_val)\n",
    "            val_lrap = self.lrap(y_pred, y_binary_val)\n",
    "\n",
    "            self.log(\n",
    "                \"val_loss\",\n",
    "                val_loss,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"val_acc_1\",\n",
    "                val_accuracy_top1,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"val_acc_2\",\n",
    "                val_accuracy_top2,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"val_f1_weighted_ignore\",\n",
    "                val_f1_weighted,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"val_f1_macro_ignore\",\n",
    "                val_f1_macro,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"val_auroc\",\n",
    "                val_auroc,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"val_lrap\",\n",
    "                val_lrap,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "\n",
    "            return val_loss\n",
    "\n",
    "        def on_train_epoch_end(self):\n",
    "            metrics = self.trainer.progress_bar_callback.get_metrics(trainer, model)\n",
    "            metrics.pop(\"v_num\", None)\n",
    "            metrics.pop(\"train_loss_step\", None)\n",
    "            for key, value in metrics.items():\n",
    "                metrics[key] = round(value, 5)\n",
    "            logger.info(f\"Epoch {self.trainer.current_epoch}: {metrics}\")\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                params=self.parameters(),\n",
    "                lr=cfg.lr_max,\n",
    "                weight_decay=cfg.weight_decay,\n",
    "                fused=cfg.fused_adamw,\n",
    "            )\n",
    "            lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=cfg.n_epochs, T_mult=1, eta_min=cfg.lr_min, last_epoch=-1\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": lr_scheduler,\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "\n",
    "    csv_logger = None\n",
    "    if not cfg.debug_run:\n",
    "        os.environ[\"PJRT_DEVICE\"] = \"GPU\"  # fix for G Cloud to avoid XLA/autocast clash\n",
    "        csv_logger = L.pytorch.loggers.CSVLogger(save_dir=\"../logs/\")\n",
    "        csv_logger.log_hyperparams(config_dictionary)\n",
    "\n",
    "    progress_bar = TQDMProgressBar(process_position=1)\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0.001,\n",
    "        patience=cfg.patience,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    model = EfficientViT()\n",
    "    trainer = L.Trainer(\n",
    "        fast_dev_run=False,\n",
    "        enable_model_summary=True,\n",
    "        max_epochs=cfg.n_epochs,\n",
    "        accelerator=cfg.accelerator,\n",
    "        precision=cfg.precision,\n",
    "        callbacks=[progress_bar, early_stopping],\n",
    "        logger=csv_logger,\n",
    "        log_every_n_steps=10,\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "        ckpt_path=None,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger.info(\"Finished training\")\n",
    "    if not cfg.debug_run:\n",
    "        logger.info(\"Saving model\")\n",
    "        filename = f\"{cfg.run_tag}_epochs_{trainer.current_epoch}.ckpt\"\n",
    "        trainer.save_checkpoint(f\"../model_objects/{filename}\")\n",
    "        logger.info(f\"Saved model to filename: {filename}\")\n",
    "    logger.info(\"All done\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
