{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# !pip install lightning timm librosa albumentations torchaudio==2.2.2 audiomentations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "\n",
    "import albumentations\n",
    "import librosa\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import timm\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchmetrics\n",
    "from joblib import Parallel, delayed\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedGroupKFold,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from losses import FocalCosineLoss, FocalLoss, FocalLossBCE, JsdCrossEntropy\n",
    "from optimizers import Adan, Nadam, NvidiaNovoGrad\n",
    "\n",
    "sns.set(\n",
    "    rc={\n",
    "        \"figure.figsize\": (8, 4),\n",
    "        \"figure.dpi\": 240,\n",
    "    }\n",
    ")\n",
    "sns.set_style(\"darkgrid\", {\"axes.grid\": False})\n",
    "sns.set_context(\"paper\", font_scale=0.6)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class cfg:\n",
    "    experiment_name = \"efnetv2s\"\n",
    "    data_path = \"../data\"\n",
    "\n",
    "    debug_run = False  # run on a small sample\n",
    "    experiment_run = False  # run on a stratified data sample\n",
    "    production_run = True  # run on all data\n",
    "\n",
    "    normalize_waveform = False  # TODO test with and without\n",
    "    sample_rate = 32000\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "    window_length = None\n",
    "    melspec_hres = 128\n",
    "    melspec_wres = 312\n",
    "    freq_min = 40\n",
    "    freq_max = 15000\n",
    "    log_scale_power = 2\n",
    "    max_decibels = 100\n",
    "    frame_duration = 5\n",
    "    frame_rate = sample_rate / hop_length\n",
    "\n",
    "    # vit_b0 = \"efficientvit_b0.r224_in1k\"\n",
    "    # vit_b1 = \"efficientvit_b1.r224_in1k\"\n",
    "    # vit_b1 = \"efficientvit_b1.r288_in1k\"\n",
    "    # vit_b2 = \"efficientvit_b2.r224_in1k\"\n",
    "    # efnet_b0 = \"tf_efficientnet_b0.in1k\"\n",
    "    # efnetv2_b0 = \"tf_efficientnetv2_b0.in1k\"\n",
    "    efnetv2s = \"tf_efficientnetv2_s.in21k_ft_in1k\"  # TODO\n",
    "    # efnet_b0_jft = \"tf_efficientnet_b0.ns_jft_in1k\" # TODO\n",
    "    # efnetv2_b1 = \"tf_efficientnetv2_b1.in1k\"\n",
    "    # vit_m0 = \"efficientvit_m0.r224_in1k\"\n",
    "    # vit_m1 = \"efficientvit_m1.r224_in1k\"\n",
    "    backbone = efnetv2s\n",
    "\n",
    "    num_classes = 182\n",
    "    mixup = True\n",
    "    mixup_alpha = 3\n",
    "    augment_melspec = True\n",
    "    add_secondary_labels = False\n",
    "    add_secondary_label_weight = 0.33\n",
    "\n",
    "    label_smoothing = 0.1\n",
    "    weighted_sampling = False\n",
    "    sample_weight_factor = 0.5\n",
    "\n",
    "    accelerator = \"gpu\"\n",
    "    precision = \"bf16-mixed\"\n",
    "    n_workers = os.cpu_count() - 2\n",
    "\n",
    "    n_epochs = 50\n",
    "    batch_size = 128\n",
    "    val_ratio = 0.20\n",
    "    patience = 10\n",
    "\n",
    "    lr = 1e-3\n",
    "    lr_min = 1e-6\n",
    "    weight_decay = 1e-3\n",
    "\n",
    "    loss = \"FocalBCE\"\n",
    "    optimizer = \"AdamW\"\n",
    "\n",
    "    timestamp = datetime.now().replace(microsecond=0)\n",
    "    run_tag = f\"{timestamp}_{backbone}_{experiment_name}_val_{val_ratio}_{loss}_{optimizer}_lr_{lr}_decay_{weight_decay}\"\n",
    "\n",
    "    if debug_run:\n",
    "        run_tag = f\"{timestamp}_{backbone}_debug\"\n",
    "        # accelerator = \"cpu\"\n",
    "        n_epochs = 5\n",
    "        batch_size = 32\n",
    "        num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def define_logger():\n",
    "    handlers = [\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(f\"../logs/{cfg.run_tag}.log\"),\n",
    "    ]\n",
    "\n",
    "    if cfg.debug_run:\n",
    "        handlers = [logging.StreamHandler()]\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\" %(asctime)s [%(threadName)s] ðŸ¦â€ðŸ”¥ %(message)s\",\n",
    "        handlers=handlers,\n",
    "        force=True,  # reconfigure root logger, in case of rerunning -> ensures new file\n",
    "    )\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_config(cfg) -> None:\n",
    "    cfg_dictionary = {\n",
    "        key: value\n",
    "        for key, value in cfg.__dict__.items()\n",
    "        if not key.startswith(\"__\") and not callable(key)\n",
    "    }\n",
    "    logger.info(f\"{'â€”' * 80}\")\n",
    "    logger.info(f\"Config: \\n{pformat(cfg_dictionary, indent=1)}\")\n",
    "    return cfg_dictionary\n",
    "\n",
    "\n",
    "def load_metadata(data_path: str) -> pd.DataFrame:\n",
    "    logger.info(f\"Loading prepared dataframes from {data_path}\")\n",
    "    model_input_df = pd.read_csv(f\"{data_path}/model_input_df.csv\")\n",
    "    sample_submission = pd.read_csv(f\"{data_path}/sample_submission.csv\")\n",
    "\n",
    "    if cfg.debug_run:\n",
    "        logger.info(\"Running debug: sampling data to 10 species and 250 samples\")\n",
    "        top_10_labels = model_input_df[\"primary_label\"].value_counts()[0:10].index\n",
    "        model_input_df = model_input_df[\n",
    "            model_input_df[\"primary_label\"].isin(top_10_labels)\n",
    "        ]\n",
    "        model_input_df = model_input_df.sample(1000).reset_index(drop=True)\n",
    "\n",
    "    elif cfg.experiment_run:\n",
    "        logger.info(\"Running experiment: sampling data\")\n",
    "        model_input_df = model_input_df.sample(frac=0.1).reset_index(drop=True)\n",
    "\n",
    "    elif cfg.production_run:\n",
    "        logger.info(\"Running production: full data\")\n",
    "        model_input_df = model_input_df.sample(frac=1, random_state=7).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "    logger.info(f\"Dataframe shape: {model_input_df.shape}\")\n",
    "\n",
    "    return model_input_df, sample_submission\n",
    "\n",
    "\n",
    "def undersample_top_birds(\n",
    "    model_input_df: pd.DataFrame, top_k: int = 20\n",
    ") -> pd.DataFrame:\n",
    "    top_k_birds = model_input_df[\"primary_label\"].value_counts()[0:top_k].index\n",
    "    top_k_birds_idx = model_input_df[model_input_df[\"primary_label\"].isin(top_k_birds)][\n",
    "        [\"sample_index\", \"primary_label\"]\n",
    "    ].index\n",
    "\n",
    "    idx_to_keep = (\n",
    "        model_input_df[model_input_df[\"primary_label\"].isin(top_k_birds)][\n",
    "            [\"sample_index\", \"primary_label\"]\n",
    "        ]\n",
    "        .drop_duplicates()\n",
    "        .index\n",
    "    )\n",
    "    idx_to_drop = top_k_birds_idx[~top_k_birds_idx.isin(idx_to_keep)]\n",
    "\n",
    "    model_input_df = model_input_df[~model_input_df.index.isin(idx_to_drop)]\n",
    "    logger.info(f\"Dataframe shape after undersampling: {model_input_df.shape}\")\n",
    "\n",
    "    return model_input_df\n",
    "\n",
    "\n",
    "def add_sample_weights(\n",
    "    model_input_df: pd.DataFrame, weight_factor: float = cfg.sample_weight_factor\n",
    ") -> pd.DataFrame:\n",
    "    sample_weights = round(\n",
    "        (\n",
    "            model_input_df[\"primary_label\"].value_counts()\n",
    "            / model_input_df[\"primary_label\"].value_counts().sum()\n",
    "        )\n",
    "        ** (-weight_factor)\n",
    "    )\n",
    "    sample_weights = pd.DataFrame(\n",
    "        {\n",
    "            \"primary_label\": sample_weights.index,\n",
    "            \"sample_weight\": sample_weights.values.astype(int),\n",
    "        }\n",
    "    )\n",
    "    model_input_df = model_input_df.merge(\n",
    "        sample_weights, on=\"primary_label\", how=\"left\"\n",
    "    )\n",
    "    return model_input_df\n",
    "\n",
    "\n",
    "def read_waveform(filename: str) -> np.ndarray:\n",
    "    filepath = f\"{cfg.data_path}/train_windows/{filename}\"\n",
    "    waveform, _ = librosa.load(filepath, sr=cfg.sample_rate)\n",
    "\n",
    "    if cfg.normalize_waveform:\n",
    "        waveform = librosa.util.normalize(waveform)\n",
    "\n",
    "    return waveform\n",
    "\n",
    "\n",
    "def read_waveforms_parallel(model_input_df: pd.DataFrame):\n",
    "    logger.info(\"Parallel Loading waveforms\")\n",
    "    waveforms = Parallel(n_jobs=cfg.n_workers, prefer=\"threads\")(\n",
    "        delayed(read_waveform)(filename)\n",
    "        for filename in tqdm(model_input_df[\"window_filename\"], desc=\"Loading waves\")\n",
    "    )\n",
    "    logger.info(\"Finished loadeding waveforms\")\n",
    "    return waveforms\n",
    "\n",
    "\n",
    "def create_label_map(submission_df: pd.DataFrame) -> dict:\n",
    "    logging.info(\"Creating label mappings\")\n",
    "    cfg.labels = submission_df.columns[1:]\n",
    "    cfg.num_classes = len(cfg.labels)\n",
    "    class_to_label_map = dict(zip(cfg.labels, np.arange(cfg.num_classes)))\n",
    "\n",
    "    return class_to_label_map\n",
    "\n",
    "\n",
    "def pad_or_crop_waveforms(waveforms: list, pad_method: str = \"repeat\") -> list:\n",
    "    logging.info(\"Padding or cropping waveforms to desired duration\")\n",
    "    desired_length = cfg.sample_rate * cfg.frame_duration\n",
    "\n",
    "    def _pad_or_crop(waveform: np.ndarray) -> np.ndarray:\n",
    "        length = len(waveform)\n",
    "\n",
    "        while length < desired_length:  # repeat if waveform too small\n",
    "            repeat_length = desired_length - length\n",
    "            padding_array = waveform[:repeat_length]\n",
    "            if pad_method != \"repeat\":\n",
    "                padding_array = np.zeros(shape=waveform[:repeat_length].shape)\n",
    "            waveform = np.concatenate([waveform, padding_array])\n",
    "            length = len(waveform)\n",
    "\n",
    "        if length > desired_length:  # crop if waveform is too big\n",
    "            offset = np.random.randint(0, length - desired_length)\n",
    "            waveform = waveform[offset : offset + desired_length]\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    waveforms = [_pad_or_crop(wave) for wave in tqdm(waveforms, desc=\"Padding waves\")]\n",
    "\n",
    "    return waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        waveforms: list,\n",
    "        add_secondary_labels: bool = cfg.add_secondary_labels,\n",
    "        augmentation: list = None,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.waveforms = waveforms\n",
    "        self.num_classes = cfg.num_classes\n",
    "        self.class_to_label_map = class_to_label_map\n",
    "        self.add_secondary_labels = add_secondary_labels\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=cfg.sample_rate,\n",
    "            n_mels=cfg.melspec_hres,\n",
    "            f_min=cfg.freq_min,\n",
    "            f_max=cfg.freq_max,\n",
    "            n_fft=cfg.n_fft,\n",
    "            hop_length=cfg.hop_length,\n",
    "            normalized=cfg.normalize_waveform,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            norm=\"slaney\",\n",
    "            mel_scale=\"slaney\",\n",
    "        )\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=cfg.max_decibels\n",
    "        )\n",
    "\n",
    "    def create_target(\n",
    "        self,\n",
    "        primary_label: str,\n",
    "        secondary_labels: list,\n",
    "        secondary_label_weight: float = cfg.add_secondary_label_weight,\n",
    "    ) -> torch.tensor:\n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        # primary_target = torch.tensor(0, dtype=torch.int64)\n",
    "\n",
    "        if primary_label != \"nocall\":\n",
    "            primary_label = self.class_to_label_map[primary_label]\n",
    "            target[primary_label] = 1\n",
    "            primary_target = torch.tensor(primary_label, dtype=torch.int64)\n",
    "\n",
    "            if self.add_secondary_labels:\n",
    "                secondary_labels = eval(secondary_labels)\n",
    "                for label in secondary_labels:\n",
    "                    if label != \"\" and label in self.class_to_label_map.keys():\n",
    "                        target[self.class_to_label_map[label]] = secondary_label_weight\n",
    "\n",
    "        return target, primary_target\n",
    "\n",
    "    def pad_or_crop_wave(\n",
    "        self, waveform: np.ndarray, pad_method: str = \"repeat\"\n",
    "    ) -> np.ndarray:\n",
    "        desired_length = cfg.sample_rate * cfg.frame_duration\n",
    "        length = len(waveform)\n",
    "\n",
    "        while length < desired_length:  # repeat if waveform too small\n",
    "            repeat_length = desired_length - length\n",
    "            padding_array = waveform[:repeat_length]\n",
    "            if pad_method != \"repeat\":\n",
    "                padding_array = np.zeros(shape=waveform[:repeat_length].shape)\n",
    "            waveform = np.concatenate([waveform, padding_array])\n",
    "            length = len(waveform)\n",
    "\n",
    "        if length > desired_length:  # crop if waveform is too big\n",
    "            offset = np.random.randint(0, length - desired_length)\n",
    "            waveform = waveform[offset : offset + desired_length]\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waveforms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform = self.waveforms[idx]\n",
    "        primary_label = self.df.iloc[idx][\"primary_label\"]\n",
    "        secondary_labels = self.df.iloc[idx][\"secondary_labels\"]\n",
    "\n",
    "        waveform = self.pad_or_crop_wave(waveform)\n",
    "        waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "\n",
    "        target, primary_target = self.create_target(\n",
    "            primary_label=primary_label, secondary_labels=secondary_labels\n",
    "        )\n",
    "\n",
    "        melspec = self.db_transform(self.mel_transform(waveform)).to(torch.uint8)\n",
    "        melspec = melspec.expand(3, -1, -1).permute(1, 2, 0).numpy()\n",
    "\n",
    "        melspec = melspec - melspec.min()\n",
    "        melspec = melspec / melspec.max()\n",
    "        melspec = melspec.astype(np.float32)\n",
    "\n",
    "        if self.augmentation is not None:\n",
    "            melspec = self.augmentation(image=melspec)[\"image\"]\n",
    "\n",
    "        return melspec, target, primary_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GlobalPool(torch.nn.Module):\n",
    "    def __init__(self, p: int = 3, eps: float = 1e-6):\n",
    "        super(GlobalPool, self).__init__()\n",
    "        self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, ch, h, w = x.shape\n",
    "        x = torch.nn.functional.avg_pool2d(\n",
    "            x.clamp(min=self.eps).pow(self.p), (x.size(-2), x.size(-1))\n",
    "        ).pow(1.0 / self.p)\n",
    "        x = x.view(bs, ch)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientNetV2(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalize = transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "        out_indices = (3, 4)\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.backbone,\n",
    "            features_only=True,\n",
    "            pretrained=True,\n",
    "            in_chans=3,\n",
    "            num_classes=cfg.num_classes,\n",
    "            out_indices=out_indices,\n",
    "        )\n",
    "        self.in_features = int(np.sum(self.backbone.feature_info.channels()))\n",
    "        print(f\"in_features: {self.in_features}\")\n",
    "\n",
    "        self.global_pools = torch.nn.ModuleList([GlobalPool() for _ in out_indices])\n",
    "        self.neck = torch.nn.BatchNorm1d(self.in_features)\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.in_features, out_features=256),\n",
    "            torch.nn.Hardswish(inplace=True),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(in_features=256, out_features=cfg.num_classes),\n",
    "        )\n",
    "\n",
    "        # self.loss_function = FocalCosineLoss()\n",
    "        self.loss_function = FocalLossBCE()\n",
    "        # self.loss_function = FocalLoss()\n",
    "        # self.loss_function = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        # self.loss_function = torch.nn.CrossEntropyLoss(\n",
    "        #     label_smoothing=cfg.label_smoothing\n",
    "        # )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\", num_classes=cfg.num_classes, top_k=1\n",
    "        )\n",
    "        self.accuracy_2 = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\", num_classes=cfg.num_classes, top_k=2\n",
    "        )\n",
    "        self.auroc = torchmetrics.AUROC(\n",
    "            task=\"multilabel\",\n",
    "            num_labels=cfg.num_classes,\n",
    "            average=\"macro\",\n",
    "        )\n",
    "        self.f1_macro = torchmetrics.F1Score(\n",
    "            task=\"multilabel\",\n",
    "            num_labels=cfg.num_classes,\n",
    "            average=\"macro\",\n",
    "            threshold=0.5,\n",
    "        )\n",
    "        self.f1_weighted = torchmetrics.F1Score(\n",
    "            task=\"multilabel\",\n",
    "            num_labels=cfg.num_classes,\n",
    "            average=\"weighted\",\n",
    "            threshold=0.5,\n",
    "        )\n",
    "        self.lrap = torchmetrics.classification.MultilabelRankingAveragePrecision(\n",
    "            num_labels=cfg.num_classes,\n",
    "        )\n",
    "        self.precision_macro = torchmetrics.classification.MultilabelPrecision(\n",
    "            num_labels=cfg.num_classes,\n",
    "            average=\"macro\",\n",
    "            threshold=0.5,\n",
    "        )\n",
    "        self.precision_weighted = torchmetrics.classification.MultilabelPrecision(\n",
    "            num_labels=cfg.num_classes,\n",
    "            average=\"weighted\",\n",
    "            threshold=0.5,\n",
    "        )\n",
    "        self.recall_macro = torchmetrics.classification.MultilabelRecall(\n",
    "            num_labels=cfg.num_classes,\n",
    "            average=\"macro\",\n",
    "            threshold=0.5,\n",
    "        )\n",
    "        self.recall_weighted = torchmetrics.classification.MultilabelRecall(\n",
    "            num_labels=cfg.num_classes,\n",
    "            average=\"weighted\",\n",
    "            threshold=0.5,\n",
    "        )\n",
    "\n",
    "    def normal_mixup(self, melspec, target, alpha=cfg.mixup_alpha):\n",
    "        indices = torch.randperm(melspec.size(0))\n",
    "        mix_melspec = melspec[indices]\n",
    "        mix_target = target[indices]\n",
    "\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        if lam < 0.5:\n",
    "            lam = 1 - lam\n",
    "\n",
    "        melspec = melspec * lam + mix_melspec * (1 - lam)\n",
    "        target = target * lam + mix_target * (1 - lam)\n",
    "\n",
    "        return melspec, target\n",
    "\n",
    "    # def time_mixup(self, melspec, target):\n",
    "    #     indices = torch.randperm(melspec.size(0))\n",
    "    #     mix_melspec = melspec[indices]\n",
    "    #     mix_target = target[indices]\n",
    "\n",
    "    #     melspec = torch.cat([melspec[:, :, :156, :], mix_melspec[:, :, 157:, :]], dim=2)\n",
    "    #     target = target + mix_target\n",
    "    #     target = torch.clamp(target, max=1)\n",
    "\n",
    "    #     return melspec, target\n",
    "\n",
    "    # def freq_mixup(self, melspec, target):\n",
    "    #     indices = torch.randperm(melspec.size(0))\n",
    "    #     mix_melspec = melspec[indices]\n",
    "    #     mix_target = target[indices]\n",
    "\n",
    "    #     melspec = torch.cat([melspec[:, :64, :, :], mix_melspec[:, 64:, :, :]], dim=1)\n",
    "    #     target = target + mix_target\n",
    "    #     target = torch.clamp(target, max=1)\n",
    "\n",
    "    #     return melspec, target\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.normalize(x)\n",
    "        x = self.backbone(x)\n",
    "        pooled_features = [self.global_pools[0](x[0]), self.global_pools[1](x[1])]\n",
    "        x = torch.cat(pooled_features, dim=1)\n",
    "        x = self.neck(x)\n",
    "        out = self.head(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, y_primary = batch\n",
    "\n",
    "        if cfg.mixup:\n",
    "            choice = np.random.choice([\"normal\", \"none\"])\n",
    "            if choice == \"normal\":\n",
    "                x, y = self.normal_mixup(x, y)\n",
    "\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_function(y_pred, y)\n",
    "\n",
    "        y_pred = y_pred.sigmoid()\n",
    "        y_int = y.to(torch.int64)\n",
    "\n",
    "        train_accuracy = self.accuracy(y_pred, y_primary)\n",
    "        train_accuracy_2 = self.accuracy_2(y_pred, y_primary)\n",
    "        train_f1_weighted = self.f1_weighted(y_pred, y_int)\n",
    "        train_f1_macro = self.f1_macro(y_pred, y_int)\n",
    "        train_lrap = self.lrap(y_pred, y_int)\n",
    "\n",
    "        train_precision_macro = self.precision_macro(y_pred, y_int)\n",
    "        train_precision_weighted = self.precision_weighted(y_pred, y_int)\n",
    "        train_recall_macro = self.recall_macro(y_pred, y_int)\n",
    "        train_recall_weighted = self.recall_weighted(y_pred, y_int)\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_accuracy\",\n",
    "            train_accuracy,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_accuracy_2\",\n",
    "            train_accuracy_2,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_f1_weighted\",\n",
    "            train_f1_weighted,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_f1_macro\",\n",
    "            train_f1_macro,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_lrap\",\n",
    "            train_lrap,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_precision_macro\",\n",
    "            train_precision_macro,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_precision_weighted\",\n",
    "            train_precision_weighted,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_recall_macro\",\n",
    "            train_recall_macro,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_recall_weighted\",\n",
    "            train_recall_weighted,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_val, y_val, y_primary_val = batch\n",
    "        y_pred = self(x_val)\n",
    "        val_loss = self.loss_function(y_pred, y_val)\n",
    "\n",
    "        y_pred = y_pred.sigmoid()\n",
    "        y_val_int = y_val.to(torch.int64)\n",
    "\n",
    "        val_accuracy = self.accuracy(y_pred, y_primary_val)\n",
    "        val_accuracy_2 = self.accuracy_2(y_pred, y_primary_val)\n",
    "        val_auroc = self.auroc(y_pred, y_val_int)\n",
    "        val_f1_weighted = self.f1_weighted(y_pred, y_val_int)\n",
    "        val_f1_macro = self.f1_macro(y_pred, y_val_int)\n",
    "        val_lrap = self.lrap(y_pred, y_val_int)\n",
    "\n",
    "        val_precision_macro = self.precision_macro(y_pred, y_val_int)\n",
    "        val_precision_weighted = self.precision_weighted(y_pred, y_val_int)\n",
    "        val_recall_macro = self.recall_macro(y_pred, y_val_int)\n",
    "        val_recall_weighted = self.recall_weighted(y_pred, y_val_int)\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            val_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_accuracy\",\n",
    "            val_accuracy,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_accuracy_2\",\n",
    "            val_accuracy_2,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_auroc\",\n",
    "            val_auroc,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_f1_weighted\",\n",
    "            val_f1_weighted,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_f1_macro\",\n",
    "            val_f1_macro,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_lrap\",\n",
    "            val_lrap,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_precision_macro\",\n",
    "            val_precision_macro,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_precision_weighted\",\n",
    "            val_precision_weighted,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_recall_macro\",\n",
    "            val_recall_macro,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_recall_weighted\",\n",
    "            val_recall_weighted,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        metrics = self.trainer.progress_bar_callback.get_metrics(trainer, model)\n",
    "        metrics.pop(\"v_num\", None)\n",
    "        metrics.pop(\"train_loss_step\", None)\n",
    "        for key, value in metrics.items():\n",
    "            metrics[key] = round(value, 5)\n",
    "        logger.info(f\"Epoch {self.trainer.current_epoch}: {metrics}\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = Adan(\n",
    "        #     model.parameters(),\n",
    "        #     lr=cfg.lr,\n",
    "        #     betas=(0.02, 0.08, 0.01),\n",
    "        #     weight_decay=cfg.weight_decay,\n",
    "        # )\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            params=self.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            fused=False,\n",
    "        )\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=cfg.n_epochs, T_mult=1, eta_min=cfg.lr_min, last_epoch=-1\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"monitor\": \"train_lrap\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger = define_logger()\n",
    "    config_dictionary = get_config(cfg)\n",
    "\n",
    "    csv_logger = None\n",
    "    if not cfg.debug_run:\n",
    "        csv_logger = L.pytorch.loggers.CSVLogger(save_dir=\"../logs/\")\n",
    "        csv_logger.log_hyperparams(config_dictionary)\n",
    "\n",
    "    model_input_df, sample_submission = load_metadata(data_path=cfg.data_path)\n",
    "    model_input_df = undersample_top_birds(model_input_df)\n",
    "    model_input_df = add_sample_weights(model_input_df)\n",
    "    class_to_label_map = create_label_map(submission_df=sample_submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-06-10 07:50:51,988 [MainThread] ðŸ¦â€ðŸ”¥ Parallel Loading waveforms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dde97645c234729997640a8f2d8cbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading waves:   0%|          | 0/83898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    waveforms = read_waveforms_parallel(model_input_df=model_input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    if cfg.augment_melspec:\n",
    "        train_augmentation = albumentations.Compose(\n",
    "            [\n",
    "                # albumentations.AdvancedBlur(p=0.20),\n",
    "                # albumentations.GaussNoise(p=0.20),\n",
    "                # albumentations.ImageCompression(\n",
    "                #     quality_lower=80, quality_upper=100, p=0.20\n",
    "                # ),\n",
    "                albumentations.CoarseDropout(\n",
    "                    max_holes=1,\n",
    "                    min_height=16,\n",
    "                    min_width=16,\n",
    "                    max_height=48,\n",
    "                    max_width=48,\n",
    "                    p=0.20,\n",
    "                ),\n",
    "                albumentations.XYMasking(\n",
    "                    p=0.20,\n",
    "                    num_masks_x=1,\n",
    "                    num_masks_y=1,\n",
    "                    mask_x_length=(3, 20),\n",
    "                    mask_y_length=(3, 20),\n",
    "                ),\n",
    "                # albumentations.RandomGridShuffle(grid=(2, 2), p=0.20),\n",
    "                # albumentations.Downscale(\n",
    "                #     scale_min=0.2, scale_max=0.9, interpolation=4, p=0.10\n",
    "                # ),\n",
    "                # albumentations.Normalize(p=1),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        train_augmentation = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    # grouped split on sample index to keep different windows from the same sample\n",
    "    # together if splitting randomly this can be considered as a form of leakage\n",
    "    # validating on a windowed waveform while windows of the same waveform were used for\n",
    "    # training is easier than classifying a waveform from a different sample, which is\n",
    "    # the case in practice\n",
    "    logger.info(f\"Splitting {len(waveforms)} waveforms into train/val: {cfg.val_ratio}\")\n",
    "    n_splits = int(round(1 / cfg.val_ratio))\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    for fold_index, (train_index, val_index) in enumerate(\n",
    "        kfold.split(\n",
    "            X=model_input_df,\n",
    "            y=model_input_df[\"primary_label\"],\n",
    "            groups=model_input_df[\"sample_index\"],\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        train_df = model_input_df.iloc[train_index]\n",
    "        val_df = model_input_df.iloc[val_index]\n",
    "\n",
    "        train_waveforms = [waveforms[i] for i in train_index]\n",
    "        val_waveforms = [waveforms[i] for i in val_index]\n",
    "\n",
    "        train_dataset = BirdDataset(\n",
    "            df=train_df, waveforms=train_waveforms, augmentation=train_augmentation\n",
    "        )\n",
    "        val_dataset = BirdDataset(df=val_df, waveforms=val_waveforms)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=cfg.batch_size,\n",
    "            drop_last=True,\n",
    "            num_workers=cfg.n_workers,\n",
    "            persistent_workers=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        if cfg.weighted_sampling:\n",
    "            logger.info(\n",
    "                f\"Defining weighted sampling with  factor: {cfg.sample_weight_factor}\"\n",
    "            )\n",
    "            sample_weight = train_df[\"sample_weight\"].values\n",
    "            sample_weight = torch.from_numpy(sample_weight)\n",
    "\n",
    "            weighted_sampler = WeightedRandomSampler(\n",
    "                sample_weight.type(\"torch.DoubleTensor\"),\n",
    "                len(sample_weight),\n",
    "                replacement=True,\n",
    "            )\n",
    "\n",
    "            train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=cfg.batch_size,\n",
    "                sampler=weighted_sampler,\n",
    "                drop_last=True,\n",
    "                num_workers=cfg.n_workers,\n",
    "                persistent_workers=True,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            num_workers=cfg.n_workers,\n",
    "            persistent_workers=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        logger.info(\"Dataloaders ready to go brrr\")\n",
    "\n",
    "        progress_bar = TQDMProgressBar(process_position=1)\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"train_lrap\",\n",
    "            min_delta=0.01,\n",
    "            patience=cfg.patience,\n",
    "            verbose=True,\n",
    "            mode=\"max\",\n",
    "        )\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            monitor=\"train_lrap\",\n",
    "            every_n_epochs=1,\n",
    "            mode=\"max\",\n",
    "            auto_insert_metric_name=True,\n",
    "            filename=f\"{cfg.run_tag}\"\n",
    "            + f\"_fold_{fold_index}_\"\n",
    "            + \"{epoch}-{val_lrap:.3f}-{val_acc2:.3f}-{val_acc2:.3f}-{val_f1_macro:.3}\",\n",
    "            dirpath=\"../model_objects/ckpts/\",\n",
    "        )\n",
    "\n",
    "        os.environ[\"PJRT_DEVICE\"] = \"GPU\"  # fix for G Cloud to avoid XLA/autocast clash\n",
    "        model = EfficientNetV2()\n",
    "        trainer = L.Trainer(\n",
    "            fast_dev_run=False,\n",
    "            enable_model_summary=True,\n",
    "            max_epochs=cfg.n_epochs,\n",
    "            accelerator=cfg.accelerator,\n",
    "            precision=cfg.precision,\n",
    "            callbacks=[progress_bar, early_stopping, model_checkpoint],\n",
    "            logger=csv_logger,\n",
    "            log_every_n_steps=10,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"\\nStart training fold {fold_index}\")\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=val_dataloader,\n",
    "            ckpt_path=None,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Finished training fold {fold_index}\")\n",
    "        if not cfg.debug_run and trainer.current_epoch > 10:\n",
    "            logger.info(\"Saving model\")\n",
    "            filename = (\n",
    "                f\"{cfg.run_tag}_fold_{fold_index}_epochs_{trainer.current_epoch}.ckpt\"\n",
    "            )\n",
    "            trainer.save_checkpoint(f\"../model_objects/{filename}\")\n",
    "            logger.info(f\"Saved model to filename: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
